{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Word Embeddings\n",
    "\n",
    "\n",
    "The groupwork of the group number two.\n",
    "\n",
    "Source of truth: https://github.com/TurkuNLP/Deep_Learning_in_LangTech_course\n",
    "\n",
    "\n",
    "##### The group\n",
    "\n",
    "- Mikael Laine\n",
    "\n",
    "- Naren \"Puri\" Purighalla\n",
    "\n",
    "- Juho Kaasalainen\n",
    "\n",
    "- Mari Salonen\n",
    "\n",
    "\n",
    "\n",
    "## The task\n",
    "\n",
    "Three tasks are to be done for the groupwork. The groupwork is divided into three milestones.\n",
    "\n",
    "###### Milestone I\n",
    "\n",
    "```Train word2vec and vary the training parameters, especially the corpus, the window size, and the word2vec architecture (skip-gram vs. BoW). Inspect the embeddings manually using the nearest neighbor, and write up your observations. How do these models differ? For models trained on two different datasets, or with two different architectures, what is the proportion of nearest neighbors which remain unchanged? Can you see any more general trends here? Write a report on your findings. Choose any language you want for your tests.```\n",
    "\n",
    "###### Milestone II\n",
    "\n",
    "```There are many formal ways to evaluate the embeddings. Test all your models on several of these, and rank them. Do you see any general trends here? Add these findings to your report. Evaluation datasets:\n",
    "http://wordvectors.org -- web based evaluation of English similarity\n",
    "https://github.com/spyysalo/wvlib -- several English datasets and evaluation scripts\n",
    "analogy_fin and analogy_en in the data directory of the course```\n",
    "\n",
    "###### Milestone III\n",
    "\n",
    "To be released."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Milestone I: Training models with word2vec\n",
    "\n",
    "##### \n",
    "\n",
    "##### Model parameters\n",
    "\n",
    "- corpus: IMDB and Reuters\n",
    "- window size: 3, 5, 7, 9\n",
    "- architecture: CBOW and skip-gram\n",
    "\n",
    "**Worth considering:**\n",
    "\n",
    "- different starting values for weights (-alpha in Mikolov w2v)\n",
    "- different amount of iterations/epochs\n",
    "\n",
    "Models should be saved into the `models/` directory. Naming should be model-corpus-windowsize-arch, e.g. `model-imdb-5-skip`.\n",
    "\n",
    "Example usage of Mikolov's word2vec for our case (you need to modify filepaths for it to work):\n",
    "\n",
    "`./word2vec -train reuters_51cls.json.tokenized -output model-reuters-3-cbow -window 3 -cbow 1`\n",
    "\n",
    "##### Reaading the models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'h5py'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-defe19c57d02>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_from_json\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'h5py'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import h5py\n",
    "import json\n",
    "from keras.models import Model, model_from_json\n",
    "\n",
    "\n",
    "def load_model(model_name):\n",
    "    with open(model_name+\".model.json\", \"rt\") as f:\n",
    "        model=model_from_json(f.read())\n",
    "    model.load_weights(model_name+\".weights.h5\")\n",
    "    with open(model_name+\".vocabularies.json\") as f:\n",
    "        labels,vocab=json.load(f)\n",
    "    return model,labels,vocab\n",
    "\n",
    "model,labels,vocab=load_model(\"models/model-imdb-3-cbow\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Milestone II: Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Get some evaluation datasets\n",
    "\n",
    "##### Test our models on the datasets\n",
    "\n",
    "##### Ranking of the models\n",
    "\n",
    "##### Some general trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
