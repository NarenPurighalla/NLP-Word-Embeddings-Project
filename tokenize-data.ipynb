{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizing data for tmikolov/word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the JSON data\n",
    "\n",
    "This needs to be done only once. You need to change the filename variable to the corpus you wish to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'class': 'neg', 'text': \"this movie is just an excuse for the writer to make a film out of 2 failed scripts.  its characters are just an assembly of characters with cliché tragic or comic attributes the sum total of which is neurotic dialog like only woody Allen could write. woman love this because its like looking in the mirror so they will enjoy this film probably  this movies was not enjoyed by me however because there was no car chase and also the film didn't have any fights. there was also no drug lords or gang bangers. Not to mention a lack of snakes. This film had no snakes. Not my cup of tea and maybe not yours ether so think about what I have said before you find yourself watching this film.  Unless of course you resemble a female have weight issues man issues enjoy sex and the city and ally mcbeal then this is meaningful for you.\"}\n",
      "[\"this movie is just an excuse for the writer to make a film out of 2 failed scripts.  its characters are just an assembly of characters with cliché tragic or comic attributes the sum total of which is neurotic dialog like only woody Allen could write. woman love this because its like looking in the mirror so they will enjoy this film probably  this movies was not enjoyed by me however because there was no car chase and also the film didn't have any fights. there was also no drug lords or gang bangers. Not to mention a lack of snakes. This film had no snakes. Not my cup of tea and maybe not yours ether so think about what I have said before you find yourself watching this film.  Unless of course you resemble a female have weight issues man issues enjoy sex and the city and ally mcbeal then this is meaningful for you.\", \"When my 14-year-old daughter and her friends get together for movie night, there's one movie they insist on watching over and over again: You guessed it, K-911, the third installment in the highly successful K-9 franchise starring everybody's favorite TV dad, Jim Belushi.  Folks, I knew it was possible to wear out a VHS tape, but a DVD?! This has been played so often that it's starting to skip; no joke! But of course you'll have that when you own a film so charming, so brilliant.  Of course, we have to thank the one and only Tom Hanks for introducing us to the beloved Cop-Dog genre with Turner and Hooch; however, even that film doesn't measure up to the sheer excellence presented in all three K-9 movies.  Some nay-sayers say Belushi ran out of steam with this third movie in the series. Poppycock, I say. While you might suspect that a third installment - direct-to-video, at that - may not seem like something worth watching, you'd prove yourself wrong after watching this quality movie.  I won't give away the plot, but I will say that Belushi and his panting partner give their best performance yet - one that will have you HOWLING with laughter! It's a shame John Belushi isn't alive to see what great strides his brother has made in the acting world.  I highly recommend your teenage daughter introduces this film to her BFFs at her next slumber party. Don't forget the puppy chow!\"]\n",
      "['neg', 'pos']\n"
     ]
    }
   ],
   "source": [
    "filename = \"data/imdb_train.json\"\n",
    "\n",
    "import json\n",
    "import random\n",
    "with open(filename) as f:\n",
    "    data=json.load(f)\n",
    "random.shuffle(data) \n",
    "print(data[0])\n",
    "\n",
    "# We need to gather the texts, into a list\n",
    "texts=[one_example[\"text\"] for one_example in data]\n",
    "labels=[one_example[\"class\"] for one_example in data]\n",
    "print(texts[:2])\n",
    "print(labels[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the CSV data\n",
    "\n",
    "This needs to be done only once. You need to change the filename variable to the corpus you wish to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data/hatespeech.csv\n",
      "1000 comments\n",
      "2000 comments\n",
      "3000 comments\n",
      "4000 comments\n",
      "5000 comments\n",
      "6000 comments\n",
      "7000 comments\n",
      "8000 comments\n",
      "9000 comments\n",
      "10000 comments\n",
      "11000 comments\n",
      "12000 comments\n",
      "13000 comments\n",
      "14000 comments\n",
      "15000 comments\n",
      "16000 comments\n",
      "17000 comments\n",
      "18000 comments\n",
      "19000 comments\n",
      "20000 comments\n",
      "21000 comments\n",
      "22000 comments\n",
      "23000 comments\n",
      "24000 comments\n",
      "[\"!!! RT @mayasolovely: As a woman you shouldn't complain about cleaning up your house. &amp; as a man you should always take the trash out...\", '!!!!! RT @mleew17: boy dats cold...tyga dwn bad for cuffin dat hoe in the 1st place!!']\n",
      "['0', '0']\n"
     ]
    }
   ],
   "source": [
    "filename = \"data/hatespeech.csv\"\n",
    "\n",
    "import csv\n",
    "\n",
    "def read_csv(file_name):\n",
    "    print(\"Reading\", file_name)\n",
    "    texts = []\n",
    "    labels = []\n",
    "    csvfile = open(file_name, 'r')\n",
    "    for i, line in enumerate(csv.DictReader(csvfile)):\n",
    "        if i % 1000 == 999:\n",
    "            print(i+1, \"comments\")\n",
    "        texts.append(line['tweet'])\n",
    "        if 'hate_speech' in line:\n",
    "            labels.append(line['hate_speech'])\n",
    "    return (texts, labels)\n",
    "\n",
    "csvData = read_csv(filename)\n",
    "texts = csvData[0]\n",
    "labels = csvData[1]\n",
    "print(texts[0:2])\n",
    "print(labels[0:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize the data\n",
    "\n",
    "This also needs to be done only once. After this we will have a tokenized version of the data in a file called \"*filename*.tokenized\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['RT', 'mayasolovely', 'As', 'woman', 'you', 'shouldn', 'complain', 'about', 'cleaning', 'up', 'your', 'house', 'amp', 'as', 'man', 'you', 'should', 'always', 'take', 'the', 'trash', 'out']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy\n",
    "\n",
    "analyzer=CountVectorizer(lowercase=False).build_analyzer() # includes tokenizer and preprocessing\n",
    "print(analyzer(texts[0]))\n",
    "with open(filename + \".tokenized\", \"w\") as file:\n",
    "    for text in texts:\n",
    "        file.write(\" \".join(analyzer(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
